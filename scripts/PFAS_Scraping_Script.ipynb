{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Industries Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take city data for cLat/cLon here: https://simplemaps.com/data/us-cities\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "earth_radius = 3960.0\n",
    "degrees_to_radians = math.pi/180.0\n",
    "radians_to_degrees = 180.0/math.pi\n",
    "#asdasd\n",
    "\n",
    "#Importing\n",
    "industries = pd.read_csv(\"/content/drive/MyDrive/pfas_python_files/preprocessing/industries.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "uscities = pd.read_csv(\"/content/drive/MyDrive/pfas_python_files/preprocessing/uscities.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "\n",
    "\n",
    "#Cleaning Data\n",
    "industries = industries[(industries['Latitude'] != '-') & (industries['Longitude'] != '-')]\n",
    "industries['Latitude'] = pd.to_numeric(industries['Latitude'], errors='coerce')\n",
    "industries['Longitude'] = pd.to_numeric(industries['Longitude'], errors='coerce')\n",
    "industries = industries[['Latitude', 'Longitude']]\n",
    "\n",
    "uscities = uscities[['city', 'state_name', 'lat', 'lng', 'population', 'density', 'ranking']]\n",
    "uscities['Latitude'] = pd.to_numeric(uscities['lat'], errors='coerce')\n",
    "uscities['Longitude'] = pd.to_numeric(uscities['lng'], errors='coerce')\n",
    "uscities = uscities.drop(['lat', 'lng'], axis=1)\n",
    "\n",
    "\n",
    "#Global Variables\n",
    "upper_boundary_miles = 0\n",
    "lower_boundary_miles = 0\n",
    "right_boundary_miles = 0\n",
    "left_boundary_miles = 0\n",
    "\n",
    "grid_radius = 2.5\n",
    "\n",
    "\n",
    "# 2.5 miles = around 0.04578754578 longitude\n",
    "# 2.5 miles = around 0.03623188405 latitude\n",
    "\n",
    "\n",
    "def calculate_industry_density_in_each_city():\n",
    "\n",
    "    uscities['Industry Count'] = 0\n",
    "    for index, row in uscities.iterrows():\n",
    "        current_city_longitude = row['Longitude']\n",
    "        current_city_latitude = row['Latitude']\n",
    "\n",
    "        # Horizontal Boundaries\n",
    "        right_boundary_longitude = current_city_longitude + 0.04578754578\n",
    "        left_boundary_longitude = current_city_longitude - 0.04578754578\n",
    "\n",
    "        # Vertical Boundaries\n",
    "        upper_boundary_latitude = current_city_latitude + 0.03623188405\n",
    "        lower_boundary_latitude = current_city_latitude - 0.03623188405\n",
    "\n",
    "        industry_count = 0\n",
    "\n",
    "        for ind_index, ind_row in industries.iterrows():\n",
    "            current_industry_longitude = ind_row['Longitude']\n",
    "            current_industry_latitude = ind_row['Latitude']\n",
    "\n",
    "            # Check each industry point to see if it falls within the 2.5 mile radius of each city\n",
    "            if (left_boundary_longitude < current_industry_longitude < right_boundary_longitude) and \\\n",
    "               (lower_boundary_latitude < current_industry_latitude < upper_boundary_latitude):\n",
    "                industry_count += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        uscities.at[index, 'Industry Count'] = industry_count\n",
    "\n",
    "        print(\"Index:\", index)\n",
    "        print(\"City:\", row[\"city\"])\n",
    "        print(\"Industry count:\", industry_count)\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "calculate_industry_density_in_each_city()\n",
    "\n",
    "columns_to_keep = ['city', 'state_name', 'Latitude', 'Longitude', 'Industry Count']\n",
    "\n",
    "uscities = uscities[columns_to_keep]\n",
    "\n",
    "uscities.to_csv('/content/drive/MyDrive/pfas_python_files/preprocessing/PFAS_Industries_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping EJScreen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8707,
     "status": "ok",
     "timestamp": 1711378472848,
     "user": {
      "displayName": "Sameer Menghani",
      "userId": "13382994027790115084"
     },
     "user_tz": 240
    },
    "id": "iK1xbt5tx4oN",
    "outputId": "956af153-ffd1-4897-f288-4319bbac82b1"
   },
   "outputs": [],
   "source": [
    "#change min/max values and run cell to complete assignments:\n",
    "#https://docs.google.com/document/d/1MVm8UH17-1ERt5JKMV1ZecDkKyYpWPnqDKAWwcvlzfo/edit?usp=sharing\n",
    "\n",
    "min = 0\n",
    "max = 2\n",
    "\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "earth_radius = 3960.0\n",
    "degrees_to_radians = math.pi/180.0\n",
    "radians_to_degrees = 180.0/math.pi\n",
    "\n",
    "uscities = pd.read_csv(\"/preprocessing/uscities.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "uscities = uscities[['city','state_name','lat','lng','population','density','ranking']]\n",
    "uscities = uscities.rename(columns={'lat': 'Latitude', 'lng': 'Longitude'})\n",
    "\n",
    "def change_in_latitude(miles):\n",
    "\treturn (miles/earth_radius)*radians_to_degrees\n",
    "\n",
    "def change_in_longitude(latitude, miles):\n",
    "\tr = earth_radius*math.cos(latitude*degrees_to_radians)\n",
    "\treturn (miles/r)*radians_to_degrees\n",
    "\n",
    "#EJ SCREEN\n",
    "\n",
    "def collect_ej_data(cLat, cLon, rad):\n",
    "    BASE_URL = \"https://geopub.epa.gov/arcgis/rest/services/ejscreen/ejscreen_v2023_with_as_cnmi_gu_vi/MapServer/exts/EJCensusReports/GetEJScreen\"\n",
    "    data = []\n",
    "    d = requests.get(BASE_URL, params={\"namestr\" : \"\",\n",
    "                                       \"geometry\" : '{\"x\": ' + str(cLon) + ', \"y\": ' + str(cLat) + ', \"spatialReference\" : {\"wkid\" : 4326}}',\n",
    "                                       \"distance\" : rad, \"unit\" : 9035,\n",
    "                                       \"areatype\" : \"\", \"areaid\" : \"\", \"f\" : \"pjson\"})\n",
    "    data.append(d.json())\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def clean_df(ej_df, cols, str_cols):\n",
    "    ej_df = ej_df[cols].copy()\n",
    "    for col in cols:\n",
    "        ej_df[col] = ej_df[col].map(lambda x: str(x).replace('%',''))\n",
    "        ej_df[col] = ej_df[col].map(lambda x: str(x).replace('N/A',''))\n",
    "        if col not in str_cols:\n",
    "            ej_df[col] = pd.to_numeric(ej_df[col])\n",
    "    return(ej_df)\n",
    "\n",
    "\n",
    "ej_columns = [\"RAW_E_PM25\", \"RAW_E_O3\", \"RAW_E_DIESEL\", \"RAW_E_CANCER\", \"RAW_E_RESP\", \"RAW_E_TRAFFIC\", \"RAW_E_LEAD\",\n",
    "              \"RAW_E_NPL\", \"RAW_E_RMP\", \"RAW_E_TSDF\", \"RAW_E_UST\", \"RAW_E_NPDES\", \"RAW_D_INDEX\", \"RAW_D_MINOR\",\n",
    "              \"RAW_D_INCOME\", \"RAW_D_UNEMPLOYED\", \"RAW_D_LING\", \"RAW_D_LESSHS\", \"RAW_D_UNDER5\", \"RAW_D_OVER64\",\n",
    "              \"S_E_PM25\", \"S_E_O3\", \"S_E_DIESEL\", \"S_E_CANCER\", \"S_E_RESP\", \"S_E_TRAFFIC\", \"S_E_LEAD\", \"S_E_NPL\",\n",
    "              \"S_E_RMP\", \"S_E_TSDF\", \"S_E_UST\", \"S_E_NPDES\", \"S_D_INDEX\", \"S_D_MINOR\", \"S_D_INCOME\", \"S_D_UNEMPLOYED\",\n",
    "              \"S_D_LING\", \"S_D_LESSHS\", \"S_D_UNDER5\", \"S_D_OVER64\", \"S_P_PM25\", \"S_P_O3\", \"S_P_DIESEL\", \"S_P_CANCER\",\n",
    "              \"S_P_RESP\", \"S_P_TRAFFIC\", \"S_P_LEAD\", \"S_P_NPL\", \"S_P_RMP\", \"S_P_TSDF\", \"S_P_UST\", \"S_P_NPDES\",\n",
    "              \"S_E_PM25_PER\", \"S_E_O3_PER\", \"S_E_DIESEL_PER\", \"S_E_CANCER_PER\", \"S_E_RESP_PER\", \"S_E_TRAFFIC_PER\",\n",
    "              \"S_E_LEAD_PER\", \"S_E_NPL_PER\", \"S_E_RMP_PER\", \"S_E_TSDF_PER\", \"S_E_UST_PER\", \"S_E_NPDES_PER\", \"S_D_INDEX_PER\",\n",
    "              \"S_D_MINOR_PER\", \"S_D_INCOME_PER\", \"S_D_UNEMPLOYED_PER\", \"S_D_LING_PER\", \"S_D_LESSHS_PER\", \"S_D_UNDER5_PER\",\n",
    "              \"S_D_OVER64_PER\", \"N_E_PM25\", \"N_E_O3\", \"N_E_DIESEL\", \"N_E_CANCER\", \"N_E_RESP\", \"N_E_TRAFFIC\", \"N_E_LEAD\",\n",
    "              \"N_E_NPL\", \"N_E_RMP\", \"N_E_TSDF\", \"N_E_UST\", \"N_E_NPDES\", \"N_D_INDEX\", \"N_D_MINOR\", \"N_D_INCOME\",\n",
    "              \"N_D_UNEMPLOYED\", \"N_D_LING\", \"N_D_LESSHS\", \"N_D_UNDER5\", \"N_D_OVER64\", \"N_P_PM25\", \"N_P_O3\", \"N_P_DIESEL\",\n",
    "              \"N_P_CANCER\", \"N_P_RESP\", \"N_P_TRAFFIC\", \"N_P_LEAD\", \"N_P_NPL\", \"N_P_RMP\", \"N_P_TSDF\",\n",
    "              \"N_P_UST\", \"N_P_NPDES\", \"N_E_PM25_PER\", \"N_E_O3_PER\", \"N_E_DIESEL_PER\", \"N_E_CANCER_PER\",\n",
    "              \"N_E_RESP_PER\", \"N_E_TRAFFIC_PER\", \"N_E_LEAD_PER\", \"N_E_NPL_PER\", \"N_E_RMP_PER\", \"N_E_TSDF_PER\",\n",
    "              \"N_E_UST_PER\", \"N_E_NPDES_PER\", \"N_D_INDEX_PER\", \"N_D_MINOR_PER\", \"N_D_INCOME_PER\",\n",
    "              \"N_D_UNEMPLOYED_PER\", \"N_D_LING_PER\", \"N_D_LESSHS_PER\", \"N_D_UNDER5_PER\", \"N_D_OVER64_PER\",\n",
    "              \"stateAbbr\", \"stateName\", \"epaRegion\", \"totalPop\", \"NUM_NPL\", \"NUM_TSDF\"]\n",
    "\n",
    "string_cols = [\"stateAbbr\", \"stateName\", \"epaRegion\", \"totalPop\", \"NUM_NPL\", \"NUM_TSDF\"]\n",
    "\n",
    "\n",
    "def scrape(i):\n",
    "    ej_scrape_df = collect_ej_data(uscities.iloc[i]['Latitude'], uscities.iloc[i]['Longitude'], 2.8217)\n",
    "    clean_ej_scrape_df = clean_df(ej_scrape_df, ej_columns, string_cols)\n",
    "    return clean_ej_scrape_df\n",
    "\n",
    "final = pd.DataFrame()\n",
    "\n",
    "i = min\n",
    "while i < max:\n",
    "    try:\n",
    "        print(\"Current Iteration:\", i)\n",
    "        scrape_df = scrape(i)\n",
    "        if scrape_df.empty:\n",
    "            print(str(i) + \" FAILED\")\n",
    "        else:\n",
    "            scrape_df.set_index(pd.Index([i]), inplace=True)\n",
    "            print(i)\n",
    "\n",
    "        if i == min:\n",
    "            final = scrape_df\n",
    "        else:\n",
    "            final = pd.concat([final, scrape_df])\n",
    "    except:\n",
    "        scrape_df = pd.DataFrame(columns=ej_columns)\n",
    "        scrape_df = scrape_df.append(pd.Series(np.nan, index=scrape_df.columns), ignore_index=True)\n",
    "        scrape_df.set_index(pd.Index([i]), inplace=True)\n",
    "        if i == min:\n",
    "            final = scrape_df\n",
    "        else:\n",
    "            final = pd.concat([final, scrape_df])\n",
    "\n",
    "\n",
    "    #print(scrape_df)\n",
    "    i = i + 1\n",
    "\n",
    "uscities = uscities[min:max]\n",
    "\n",
    "final_df = pd.DataFrame(final)\n",
    "final_df = pd.concat([uscities, final], axis=1)\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "final_df.to_csv('/scraping/PFAS_Stats_Final_Df_Rows_' + str(min) + '_to_' + str(max - 1) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5z0p3LpxyOss"
   },
   "outputs": [],
   "source": [
    "# # the code below replaces the primary EJ Screen data file from scraping\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# combine_files = os.listdir('/scraping')\n",
    "\n",
    "# combine_files_df = []\n",
    "\n",
    "# for combine_file in combine_files:\n",
    "#     temp_df = pd.read_csv('/scraping/' + str(combine_file))\n",
    "#     temp_df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "#     temp_df.index.name = None\n",
    "#     combine_files_df.append(temp_df)\n",
    "\n",
    "# new_df = pd.concat(combine_files_df)\n",
    "# new_df = new_df.sort_index()\n",
    "# new_df.to_csv('/preprocessing/PFAS_EJ_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine EJScreen and Industry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOjhNLe_tpen"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ej_data = pd.read_csv('/preprocessing/PFAS_EJ_Data.csv')\n",
    "ej_data.set_index(\"Unnamed: 0\", inplace=True)\n",
    "ej_data.index.name = None\n",
    "\n",
    "industries_data = pd.read_csv('/preprocessing/PFAS_Industries_Data.csv')\n",
    "industries_data.set_index(\"Unnamed: 0\", inplace=True)\n",
    "industries_data.index.name = None\n",
    "\n",
    "\n",
    "final_pfas = pd.concat([industries_data, ej_data], axis=1)\n",
    "final_pfas = final_pfas.loc[:,~final_pfas.columns.duplicated()]\n",
    "\n",
    "final_pfas.insert(6, \"Industry Presence\", final_pfas[\"Industry Count\"].apply(lambda x: 0 if x == 0 else 1))\n",
    "final_pfas.to_csv(\"/preprocessing/PFAS_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take city data for cLat/cLon here: https://simplemaps.com/data/us-cities\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "earth_radius = 3960.0\n",
    "degrees_to_radians = math.pi/180.0\n",
    "radians_to_degrees = 180.0/math.pi\n",
    "#asdasd\n",
    "\n",
    "#Importing\n",
    "industries = pd.read_csv(\"/content/drive/MyDrive/pfas_python_files/preprocessing/industries.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "uscities = pd.read_csv(\"/content/drive/MyDrive/pfas_python_files/preprocessing/PFAS_Data.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "testing = pd.read_csv(\"/content/drive/MyDrive/pfas_python_files/preprocessing/testing_sites.csv\", encoding = 'unicode_escape', engine ='python')\n",
    "\n",
    "\n",
    "#Cleaning Data\n",
    "testing = testing[(testing['latitude'] != '-') & (testing['longitude'] != '-')]\n",
    "testing['latitude'] = pd.to_numeric(testing['latitude'], errors='coerce')\n",
    "testing['longitude'] = pd.to_numeric(testing['longitude'], errors='coerce')\n",
    "testing = testing[['latitude', 'longitude']]\n",
    "\n",
    "\n",
    "uscities['Latitude'] = pd.to_numeric(uscities['Latitude'], errors='coerce')\n",
    "uscities['Longitude'] = pd.to_numeric(uscities['Longitude'], errors='coerce')\n",
    "\n",
    "\n",
    "#Global Variables\n",
    "upper_boundary_miles = 0\n",
    "lower_boundary_miles = 0\n",
    "right_boundary_miles = 0\n",
    "left_boundary_miles = 0\n",
    "\n",
    "grid_radius = 2.5\n",
    "\n",
    "\n",
    "# 2.5 miles = around 0.04578754578 longitude\n",
    "# 2.5 miles = around 0.03623188405 latitude\n",
    "\n",
    "\n",
    "def calculate_testing_density_in_each_city():\n",
    "\n",
    "    uscities['Test Sites Count'] = 0\n",
    "    for index, row in uscities.iterrows():\n",
    "        current_city_longitude = row['Longitude']\n",
    "        current_city_latitude = row['Latitude']\n",
    "\n",
    "        # Horizontal Boundaries\n",
    "        right_boundary_longitude = current_city_longitude + 0.04578754578\n",
    "        left_boundary_longitude = current_city_longitude - 0.04578754578\n",
    "\n",
    "        # Vertical Boundaries\n",
    "        upper_boundary_latitude = current_city_latitude + 0.03623188405\n",
    "        lower_boundary_latitude = current_city_latitude - 0.03623188405\n",
    "\n",
    "        testing_count = 0\n",
    "\n",
    "        for ind_index, ind_row in testing.iterrows():\n",
    "            current_testing_longitude = ind_row['longitude']\n",
    "            current_testing_latitude = ind_row['latitude']\n",
    "\n",
    "            # Check each testing point to see if it falls within the 2.5 mile radius of each city\n",
    "            if (left_boundary_longitude < current_testing_longitude < right_boundary_longitude) and \\\n",
    "               (lower_boundary_latitude < current_testing_latitude < upper_boundary_latitude):\n",
    "                testing_count += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        uscities.at[index, 'Testing Count'] = testing_count\n",
    "\n",
    "        print(\"Index:\", index)\n",
    "        print(\"City:\", row[\"city\"])\n",
    "        print(\"Testing Count :\", testing_count)\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "calculate_testing_density_in_each_city()\n",
    "\n",
    "columns_to_keep = ['city', 'state_name', 'Latitude', 'Longitude', 'Testing Count']\n",
    "\n",
    "uscities = uscities[columns_to_keep]\n",
    "\n",
    "uscities.to_csv('/preprocessing/PFAS_Testing_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "17gByc9oYF8dSs_oMIInYXgKdjRkyES45",
     "timestamp": 1686607855637
    },
    {
     "file_id": "1E3dG-Y26M64Pa5hTlZSIVchSjeeAjBuU",
     "timestamp": 1683642916117
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
